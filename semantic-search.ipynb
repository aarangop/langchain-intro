{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656755ec",
   "metadata": {},
   "source": [
    "# Semantic Search\n",
    "\n",
    "This notebook demonstrates LangChain's [document loader](https://python.langchain.com/docs/concepts/document_loaders/) [embedding](https://python.langchain.com/docs/concepts/embedding_models/), and [vector store](https://python.langchain.com/docs/concepts/vectorstores/) abstractions.\n",
    "\n",
    "These are fundamental abstractions for more advanced LLM use cases, such as retrieval-augmented generation (RAG).\n",
    "\n",
    "We will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd772e",
   "metadata": {},
   "source": [
    "## Documents and Document Loaders\n",
    "\n",
    "The first abstraction we're going to use is the Document abstraction, which is intended to represent a unit of text, and associated metadata. It has three attributes:\n",
    "- `page_content`: a string representing the content\n",
    "- `metadata`: dict with arbitrary metadata \n",
    "- `id` (optional): string identifier for the document.\n",
    "\n",
    "Below, we'll generate sample documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea4daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dedaad",
   "metadata": {},
   "source": [
    "However, the LangChain *ecosystem* implements document loaders that [integrate with hundreds of common sources](https://python.langchain.com/docs/integrations/document_loaders/). This makes it easy to incorporate those sources into an AI application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519ea8a",
   "metadata": {},
   "source": [
    "### Loading Documents\n",
    "\n",
    "Below, we'll load a PDF into a sequence of `Document` objects.\n",
    "\n",
    "> See [this guide](https://python.langchain.com/docs/how_to/document_loader_pdf/) for more detail on PDF document loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5150d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "file_path = \"./data/2021_lewis_et_al..pdf\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f'File {file_path} not found')\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc05ee",
   "metadata": {},
   "source": [
    "The `PyPDFLoader` loads one `Document` object per PDF page. We can easily access the string content of the page, and the metadata containing the file name and page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d59a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4: Human assessments for the Jeopardy\n",
      "Question Generation Task.\n",
      "Factuality Speciﬁcity\n",
      "BART better 7.1% 16.8%\n",
      "RAG better 42.7% 37.4%\n",
      "Both good 11.7% 11.8%\n",
      "Both poor 17.7% 6.9%\n",
      "No majority 20.8% 2\n",
      "\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/2021_lewis_et_al..pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "page = random.choice(range(len(docs)))\n",
    "print(f\"{docs[page].page_content[:200]}\\n\")\n",
    "print(docs[page].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f31eab",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "\n",
    "For information retrieval (e.g. RAG) and downstream question-answering purposes, a page may be too coarse a representation. \n",
    "\n",
    "Our goal in the end will be to retrieve Document objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
    "\n",
    "We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”.\n",
    "\n",
    "See [this guide](https://python.langchain.com/docs/how_to/document_loader_pdf/) for more detail about working with PDFs, including how to extract text from specific sections and images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "525e9d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728bdf8",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "In order to store and search over unstructured data, we can use embeddings. Embeddings \"capture\" the semantics (meaning) of text chunks, codifying it as an embedding, a position of a multidimensional space. An embedding is another word for a vector in a multidimensional space. So, each text chunk is turned into a multidimensional vector that \"captures\" its meaning.\n",
    "\n",
    "Given a query, we can embed it using the same number of dimensions as our embeddings, and use vector similarity metrics (such as cosine similarity) to indentify related text.\n",
    "\n",
    "LangChain supports embeddings from [many providers](https://python.langchain.com/docs/integrations/text_embedding/).\n",
    "\n",
    "For our example we'll use HuggingFace embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "700f7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36533a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae24474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 3072\n",
      "\n",
      "[0.012752422131597996, 0.0023397256154567003, -0.011890682391822338, -0.021683795377612114, -0.00503349956125021, 0.03107609786093235, -0.030247757211327553, 0.020855454728007317, -0.038023460656404495, 0.009806472808122635]\n"
     ]
    }
   ],
   "source": [
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "\n",
    "print(f\"Generated vectors of length {len(vector_2)}\\n\")\n",
    "print(vector_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e7f5f",
   "metadata": {},
   "source": [
    "# Vector Store\n",
    "\n",
    "Now that we have embeddings, we can store them in special data structures that support similarity search.\n",
    "\n",
    "For this example we'll use an in-memory vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c51efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb5618",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e4449",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Now we have a vector database that we can use to perform queries relating to our PDF.\n",
    "\n",
    "The PDF that we stored in the database is the [landmark paper on RAG, by Lewis et al.](https://arxiv.org/pdf/2005.11401).\n",
    "\n",
    "\"Querying\" the database is actually very simple now. Well write a query, which will be embedded in the same vector space of the embeddings. Then we perform a similarity search, which will return the chunks that are most relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07033be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
      "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
      "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
      "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
      "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
      "seq2seq baseline.\n",
      "1 Introduction\n",
      "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\n",
      "edge from data [47]. They can do so without any access to an external memory, as a parameterized\n",
      "implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\n",
      "sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\n",
      "their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/2021_lewis_et_al..pdf', 'total_pages': 19, 'page': 0, 'page_label': '1', 'start_index': 1559}\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"What are some limitations of pre-trained language models when it comes to knowledge intensive tasks?\"\n",
    ")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f617c",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "\n",
    "LangChain `VectorStore` does not implement the `Runnable` interface, but `Retriever`s do, so they implement a standard set of methods, that allow chaining them to a LangChain application. \n",
    "\n",
    "While we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, such as external APIs.\n",
    "\n",
    "Below we create a simple version of this, without subclassing `Retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7240fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    return vector_store.similarity_search(query, k=2)\n",
    "\n",
    "results = retriever.batch([\n",
    "    \"What is the cost function that is minimized in RAG training?\",\n",
    "    \"In what tasks does the RAG approach surpass the state of the art with seq2seq models?\",\n",
    "    \"What are the advantages of RAG vs. purely parametric models?\"\n",
    "])\n",
    "\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc9b4706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('minimize the negative marginal log-likelihood of each target, ∑\\n'\n",
      " 'j−log p(yj|xj) using stochastic\\n'\n",
      " 'gradient descent with Adam [28]. Updating the document encoder BERTd during '\n",
      " 'training is costly as\\n'\n",
      " 'it requires the document index to be periodically updated as REALM does '\n",
      " 'during pre-training [20].\\n'\n",
      " 'We do not ﬁnd this step necessary for strong performance, and keep the '\n",
      " 'document encoder (and\\n'\n",
      " 'index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART '\n",
      " 'generator.\\n'\n",
      " '2.5 Decoding\\n'\n",
      " 'At test time, RAG-Sequence and RAG-Token require different ways to '\n",
      " 'approximatearg maxyp(y|x).\\n'\n",
      " 'RAG-Token The RAG-Token model can be seen as a standard, autoregressive '\n",
      " 'seq2seq genera-\\n'\n",
      " 'tor with transition probability: p′\\n'\n",
      " 'θ(yi|x,y1:i−1) = ∑\\n'\n",
      " 'z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x,zi,y1:i−1) To\\n'\n",
      " 'decode, we can plug p′\\n'\n",
      " 'θ(yi|x,y1:i−1) into a standard beam decoder.\\n'\n",
      " 'RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a '\n",
      " 'conventional per-')\n",
      "('blob/master/examples/rag/README.md and an interactive demo of a RAG model '\n",
      " 'can be found\\n'\n",
      " 'at https://huggingface.co/rag/\\n'\n",
      " '2https://github.com/pytorch/fairseq\\n'\n",
      " '3https://github.com/huggingface/transformers\\n'\n",
      " '17')\n",
      "('two variants: the standard 3-way classiﬁcation task (supports/refutes/not '\n",
      " 'enough info) and the 2-way\\n'\n",
      " '(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we '\n",
      " 'report label accuracy.\\n'\n",
      " '4 Results\\n'\n",
      " '4.1 Open-domain Question Answering\\n'\n",
      " 'Table 1 shows results for RAG along with state-of-the-art models. On all '\n",
      " 'four open-domain QA\\n'\n",
      " 'tasks, RAG sets a new state of the art (only on the T5-comparable split for '\n",
      " 'TQA). RAG combines\\n'\n",
      " 'the generation ﬂexibility of the “closed-book” (parametric only) approaches '\n",
      " 'and the performance of\\n'\n",
      " '\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys '\n",
      " 'strong results\\n'\n",
      " 'without expensive, specialized “salient span masking” pre-training [20]. It '\n",
      " 'is worth noting that RAG’s\\n'\n",
      " 'retriever is initialized using DPR’s retriever, which uses retrieval '\n",
      " 'supervision on Natural Questions\\n'\n",
      " 'and TriviaQA. RAG compares favourably to the DPR QA system, which uses a '\n",
      " 'BERT-based “cross-')\n",
      "('per token. We ﬁne-tune and evaluate our models on a wide range of '\n",
      " 'knowledge-\\n'\n",
      " 'intensive NLP tasks and set the state of the art on three open domain QA '\n",
      " 'tasks,\\n'\n",
      " 'outperforming parametric seq2seq models and task-speciﬁc '\n",
      " 'retrieve-and-extract\\n'\n",
      " 'architectures. For language generation tasks, we ﬁnd that RAG models '\n",
      " 'generate\\n'\n",
      " 'more speciﬁc, diverse and factual language than a state-of-the-art '\n",
      " 'parametric-only\\n'\n",
      " 'seq2seq baseline.\\n'\n",
      " '1 Introduction\\n'\n",
      " 'Pre-trained neural language models have been shown to learn a substantial '\n",
      " 'amount of in-depth knowl-\\n'\n",
      " 'edge from data [47]. They can do so without any access to an external '\n",
      " 'memory, as a parameterized\\n'\n",
      " 'implicit knowledge base [51, 52]. While this development is exciting, such '\n",
      " 'models do have down-\\n'\n",
      " 'sides: They cannot easily expand or revise their memory, can’t '\n",
      " 'straightforwardly provide insight into\\n'\n",
      " 'their predictions, and may produce “hallucinations” [38]. Hybrid models that '\n",
      " 'combine parametric')\n",
      "('per token. We ﬁne-tune and evaluate our models on a wide range of '\n",
      " 'knowledge-\\n'\n",
      " 'intensive NLP tasks and set the state of the art on three open domain QA '\n",
      " 'tasks,\\n'\n",
      " 'outperforming parametric seq2seq models and task-speciﬁc '\n",
      " 'retrieve-and-extract\\n'\n",
      " 'architectures. For language generation tasks, we ﬁnd that RAG models '\n",
      " 'generate\\n'\n",
      " 'more speciﬁc, diverse and factual language than a state-of-the-art '\n",
      " 'parametric-only\\n'\n",
      " 'seq2seq baseline.\\n'\n",
      " '1 Introduction\\n'\n",
      " 'Pre-trained neural language models have been shown to learn a substantial '\n",
      " 'amount of in-depth knowl-\\n'\n",
      " 'edge from data [47]. They can do so without any access to an external '\n",
      " 'memory, as a parameterized\\n'\n",
      " 'implicit knowledge base [51, 52]. While this development is exciting, such '\n",
      " 'models do have down-\\n'\n",
      " 'sides: They cannot easily expand or revise their memory, can’t '\n",
      " 'straightforwardly provide insight into\\n'\n",
      " 'their predictions, and may produce “hallucinations” [38]. Hybrid models that '\n",
      " 'combine parametric')\n",
      "('non-parametric memory is a dense vector index of Wikipedia, accessed with a '\n",
      " 'pre-trained neural\\n'\n",
      " 'retriever. We combine these components in a probabilistic model trained '\n",
      " 'end-to-end (Fig. 1). The\\n'\n",
      " 'retriever (Dense Passage Retriever [26], henceforth DPR) provides latent '\n",
      " 'documents conditioned on\\n'\n",
      " 'the input, and the seq2seq model (BART [32]) then conditions on these latent '\n",
      " 'documents together with\\n'\n",
      " 'the input to generate the output. We marginalize the latent documents with a '\n",
      " 'top-K approximation,\\n'\n",
      " 'either on a per-output basis (assuming the same document is responsible for '\n",
      " 'all tokens) or a per-token\\n'\n",
      " 'basis (where different documents are responsible for different tokens). Like '\n",
      " 'T5 [51] or BART, RAG\\n'\n",
      " 'can be ﬁne-tuned on any seq2seq task, whereby both the generator and '\n",
      " 'retriever are jointly learned.\\n'\n",
      " 'There has been extensive previous work proposing architectures to enrich '\n",
      " 'systems with non-parametric\\n'\n",
      " 'memory which are trained from scratch for speciﬁc tasks, e.g. memory '\n",
      " 'networks [ 64, 55], stack-')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for matches in results:\n",
    "    for match in matches:\n",
    "        pprint(match.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545daeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
